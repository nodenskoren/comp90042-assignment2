{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "02-assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W_oI1y6-cY4d"
      },
      "source": [
        "# Homework 2: Word Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "XIBjsAqjcY4e"
      },
      "source": [
        "Student Name: Nodens F. Koren\n",
        "\n",
        "Student ID: 1060811"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Huv7ALVcY4f"
      },
      "source": [
        "## General info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p07JVvoScY4g"
      },
      "source": [
        "<b>Due date</b>: Thursday, 4 June 2020 5pm\n",
        "\n",
        "<b>Submission method</b>: Canvas submission\n",
        "\n",
        "<b>Submission materials</b>: completed copy of this iPython notebook\n",
        "\n",
        "<b>Late submissions</b>: -10% per day (both week and weekend days counted)\n",
        "\n",
        "<b>Marks</b>: 10% of mark for class (with 9% on correctness + 1% on quality and efficiency of your code)\n",
        "\n",
        "**Note**: As we will be implementing neural networks in this assignment, you're encouraged to build your notebook on **colab**. See the programming exercise in workshop-07 (`10-bert.ipynb`) if you are not familiar with colab.\n",
        "\n",
        "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/17601/pages/using-jupyter-notebook-and-python?module_item_id=1678430) on Canvas (under Modules>Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. Deep learning libraries such as keras and pytorch are also allowed.  You can also use any Python built-in packages, but do not use any other 3rd party packages (the packages listed above are all fine to use); if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
        "\n",
        "To familiarize yourself with NLTK, here is a free online book:  Steven Bird, Ewan Klein, and Edward Loper (2009). <a href=http://nltk.org/book>Natural Language Processing with Python</a>. O'Reilly Media Inc. You may also consult the <a href=https://www.nltk.org/api/nltk.html>NLTK API</a>.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
        "\n",
        "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
        "\n",
        "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board; we recommend you check it regularly.\n",
        "\n",
        "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wseHhYGScY4g"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wH9UvbJTcY4h"
      },
      "source": [
        "In this homework, you'll be quantifying the similarity between pairs of words of a dataset using different methods with the word co-occurrence in the Brown corpus and synset structure of WordNet. Firstly, you will preprocess the dataset to filter out the rare and ambiguous words. Secondly, you will calculate the similarity scores for pairs of words in the filtered dataset using Lin similarity, NPMI and LSA. Lastly, you will quantify how well these methods work by comparing to a human annotated gold-standard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ptNKS9CcY4h"
      },
      "source": [
        "## 1. Preprocessing (2 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "V2OlvNAicY4i"
      },
      "source": [
        "### Question 1 (1.0 mark)\n",
        "\n",
        "<b>Instructions</b>: For this homework we will be comparing our methods against a popular dataset of word similarities called <a href=\"http://alfonseca.org/eng/research/wordsim353.html\">Similarity-353</a>. You need to first obtain this dataset, which is on Canvas (assignment 2). The file we will be using is called *set1.tab*. Make sure you save this in the same folder as the notebook.  Except for the header (which should be stripped out), the file is tab formated with the first two columns corresponding to two words, and the third column representing a human-annotated similarity between the two words. <b>You should ignore the subsequent columns</b>.\n",
        "\n",
        "Here shows the first six lines of the file:\n",
        "\n",
        "```\n",
        "Word 1\tWord 2\tHuman (mean)\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t\n",
        "love\tsex\t6.77\t9\t6\t8\t8\t7\t8\t8\t4\t7\t2\t6\t7\t8\t\n",
        "tiger\tcat\t7.35\t9\t7\t8\t7\t8\t9\t8.5\t5\t6\t9\t7\t5\t7\t\n",
        "tiger\ttiger\t10.00\t10\t10\t10\t10\t10\t10\t10\t10\t10\t10\t10\t10\t10\t\n",
        "book\tpaper\t7.46\t8\t8\t7\t7\t8\t9\t7\t6\t7\t8\t9\t4\t9\t\n",
        "computer\tkeyboard\t7.62\t8\t7\t9\t9\t8\t8\t7\t7\t6\t8\t10\t3\t9\t\n",
        "```\n",
        "    \n",
        "You should load this file into a Python dictionary (NOTE: in Python, tuples of strings, i.e. (\"tiger\",\"cat\") can serve as the keys of a dictionary to map to their human-annotated similarity). This dataset contains many rare words: we need to filter this dataset in order for it to be better suited to the resources we will use in this assignment. So your first goal is to filter this dataset to generate a smaller test set where you will evaluate your word similarity methods.\n",
        "\n",
        "The first filtering is based on **document frequencies** (not token frequencies) in the Brown corpus, in order to remove rare words. In this homework, **we will be treating the paragraphs of the Brown corpus as our \"documents\"**. You can iterate over them by using the `paras` method of the corpus reader. You should remove tokens that are not alphabetic. Tokens should be lower-cased and lemmatized (lemmatize function provided). Store this preprocessed data in *brown_corpus* object (it will be used for question 4 and 5 later).\n",
        "\n",
        "Now calculate document frequencies for each word type, and use this to remove from your word similarity data any word pairs where at least one of the two words has a document frequency of **$< 8$** in this corpus. You should store all the word pair and similarity mappings in your filtered test set in a dictionary called *filtered_gold_standard*.\n",
        "\n",
        "Note: the document frequency of a word denotes the number of documents that contains the word.\n",
        "\n",
        "**Task**: Filter word pairs from *set1.tab* based on document frequencies. Produce *brown_corpus*, which is a list where each element is a set of words for one paragraph (e.g. the first element in *brown_corpus* should contain all the unique word types for the first paragraph). Produce *filtered_gold_standard*, a dictionary of filtered word pairs with human similarity ratings (the dictionary should have (word1, word2) as keys, and similarity ratings as values).\n",
        "\n",
        "**Check**: Use the assertion statements in *\"For your testing\"* below for the expected *filtered_gold_standard*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szPlY9PIcY4j",
        "outputId": "5ff7c877-9be8-459a-e1e3-565bd87d39e3",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download(\"brown\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# filtered_gold_standard stores the word pairs and their human-annotated similarity in your filtered test set\n",
        "filtered_gold_standard = {}\n",
        "\n",
        "# lemmatizer\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word,'v')\n",
        "    if lemma == word:\n",
        "        lemma = lemmatizer.lemmatize(word,'n')\n",
        "    return lemma\n",
        "\n",
        "\n",
        "###\n",
        "# Your answer BEGINS HERE\n",
        "###\n",
        "\n",
        "# Filter out non-alphabetic words and lemmatize the lowercased words.\n",
        "# Remove duplicates for each paragraph by using the set data structure.\n",
        "brown_corpus = []\n",
        "for paragraph in brown.paras():\n",
        "    word_set = set()\n",
        "    for sentence in paragraph:\n",
        "        for word in sentence:\n",
        "            if word.isalpha():\n",
        "                lemma = lemmatize(word.lower())\n",
        "                word_set.add(lemma)\n",
        "    brown_corpus.append(word_set)\n",
        "\n",
        "# Reads in the raw set1.tab file and only stores the first three columns which\n",
        "# we will be using.\n",
        "unfiltered_dict = {}\n",
        "set1 = open(\"set1.tab\")\n",
        "set1.readline()\n",
        "for line in set1:\n",
        "    line = line.strip('\\n')\n",
        "    items = line.split('\\t')\n",
        "    k, v = (items[0], items[1]), float(items[2])\n",
        "    unfiltered_dict[k] = v\n",
        "\n",
        "# This helper function calculates the document frequency for each word from \n",
        "# set1.tab in the brown_corpus.\n",
        "def get_document_frequency(word, corpus):\n",
        "    count = 0\n",
        "    for document in corpus:\n",
        "        if word in document:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "# Filter out any word (key) pairs where at least one of the two words \n",
        "# has a document frequency of  <8  in the brown_corpus.\n",
        "filtered_dictionary = {}\n",
        "for k, v in unfiltered_dict.items():\n",
        "    if get_document_frequency(k[0], brown_corpus) >= 8 and get_document_frequency(k[1], brown_corpus) >= 8:\n",
        "        filtered_gold_standard[k] = v\n",
        "            \n",
        "###\n",
        "# Your answer ENDS HERE\n",
        "###\n",
        "\n",
        "print(len(filtered_gold_standard))\n",
        "print(filtered_gold_standard)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "94\n",
            "{('love', 'sex'): 6.77, ('tiger', 'cat'): 7.35, ('tiger', 'tiger'): 10.0, ('book', 'paper'): 7.46, ('plane', 'car'): 5.77, ('train', 'car'): 6.31, ('telephone', 'communication'): 7.5, ('television', 'radio'): 6.77, ('drug', 'abuse'): 6.85, ('bread', 'butter'): 6.19, ('doctor', 'nurse'): 7.0, ('professor', 'doctor'): 6.62, ('student', 'professor'): 6.81, ('smart', 'student'): 4.62, ('smart', 'stupid'): 5.81, ('company', 'stock'): 7.08, ('stock', 'market'): 8.08, ('stock', 'phone'): 1.62, ('stock', 'egg'): 1.81, ('stock', 'live'): 3.73, ('stock', 'life'): 0.92, ('book', 'library'): 7.46, ('bank', 'money'): 8.12, ('wood', 'forest'): 7.73, ('money', 'cash'): 9.08, ('king', 'queen'): 8.58, ('bishop', 'rabbi'): 6.69, ('holy', 'sex'): 1.62, ('football', 'basketball'): 6.81, ('football', 'tennis'): 6.63, ('tennis', 'racket'): 7.56, ('law', 'lawyer'): 8.38, ('movie', 'star'): 7.38, ('movie', 'critic'): 6.73, ('movie', 'theater'): 7.92, ('space', 'chemistry'): 4.88, ('alcohol', 'chemistry'): 5.54, ('drink', 'car'): 3.04, ('drink', 'ear'): 1.31, ('drink', 'mouth'): 5.96, ('drink', 'eat'): 6.87, ('baby', 'mother'): 7.85, ('drink', 'mother'): 2.65, ('car', 'automobile'): 8.94, ('journey', 'voyage'): 9.29, ('coast', 'shore'): 9.1, ('food', 'fruit'): 7.52, ('bird', 'cock'): 7.1, ('tool', 'implement'): 6.46, ('brother', 'monk'): 6.27, ('journey', 'car'): 5.85, ('coast', 'hill'): 4.38, ('forest', 'graveyard'): 1.85, ('monk', 'slave'): 0.92, ('coast', 'forest'): 3.15, ('chord', 'smile'): 0.54, ('noon', 'string'): 0.54, ('money', 'dollar'): 8.42, ('money', 'currency'): 9.04, ('money', 'wealth'): 8.27, ('money', 'property'): 7.57, ('money', 'possession'): 7.29, ('money', 'bank'): 8.5, ('money', 'deposit'): 7.73, ('money', 'operation'): 3.31, ('tiger', 'animal'): 7.0, ('tiger', 'organism'): 4.77, ('tiger', 'zoo'): 5.87, ('psychology', 'anxiety'): 7.0, ('psychology', 'fear'): 6.85, ('psychology', 'depression'): 7.42, ('psychology', 'doctor'): 6.42, ('psychology', 'mind'): 7.69, ('psychology', 'health'): 7.23, ('psychology', 'science'): 6.71, ('psychology', 'discipline'): 5.58, ('planet', 'star'): 8.45, ('planet', 'moon'): 8.08, ('planet', 'sun'): 8.02, ('planet', 'galaxy'): 8.11, ('planet', 'space'): 7.92, ('precedent', 'example'): 5.85, ('precedent', 'information'): 3.85, ('precedent', 'law'): 6.65, ('precedent', 'collection'): 2.5, ('precedent', 'group'): 1.77, ('cup', 'coffee'): 6.58, ('cup', 'article'): 2.4, ('cup', 'object'): 3.69, ('cup', 'entity'): 2.15, ('cup', 'drink'): 7.25, ('cup', 'food'): 5.0, ('cup', 'substance'): 1.92, ('cup', 'liquid'): 5.9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JTt3T9fycY4p"
      },
      "source": [
        "<b>For your testing: </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PCkSP91lcY4q",
        "colab": {}
      },
      "source": [
        "assert(len(brown_corpus)==15667)\n",
        "assert(len(filtered_gold_standard) > 50 and len(filtered_gold_standard) < 100)\n",
        "assert(filtered_gold_standard[('love', 'sex')] == 6.77)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zknFIccAcY40"
      },
      "source": [
        "### Question 2 (1.0 mark)\n",
        "\n",
        "<b>Instructions</b>: Here, you apply the second filtering. The second filtering is based on words with highly ambiguous senses and involves using the NLTK interface to WordNet. Here, you should remove any words which do not have a **single primary sense**. We define single primary sense here as either: (a) having only one sense (i.e. only one synset), or (b) where the count (as provided by the WordNet `count()` method for the lemmas associated with a synset) of the most common sense is at least 4 times larger than the next most common sense. Note that a synset can be associated with multiple lemmas. You should only consider the count of your lemma.\n",
        "\n",
        "Note: You should lowercase the lemmas of a synset when matching your word; and if there are multiple lowercased lemmas that match your word, you should sum up the count of all matching lemmas.\n",
        "\n",
        "Additionally, you should remove any words where the primary sense is **not a noun** (this information is also in the synset). Store the synset corresponding to this primary sense in a dictionary for use in the next section.\n",
        "\n",
        "Given this definition, remove the word pairs from the test set where at least one of the words does not meet the above criteria. When you have applied the two filtering steps, you should store all the word pair and similarity mappings in your filtered test set in a dictionary called *final_gold_standard*.\n",
        "\n",
        "**Task**: Filter word pairs for any words which do not have a single primary sense and aren't nouns. Produce *final_gold_standard*, a dictionary of filtered word pairs with human similarity ratings. Note: this second filtering is applied on top of the first filtering (question 1). In other words, you shouldn't consider any word pairs that have already been discarded by the first filtering in this question.\n",
        "\n",
        "**Check**: Use the assertion statements in *\"For your testing\"* for the expected *final_gold_standard*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6rdnrOXcY41",
        "outputId": "c8fb739d-79d8-4e50-c29e-0944276c67b5",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# final_gold_standard stores the word pairs and their human-annotated similarity in your final filtered test set\n",
        "final_gold_standard = {}\n",
        "word_primarysense = {} #a dictionary of (word, primary_sense) (used for next section); primary_sense is a synset\n",
        "\n",
        "###\n",
        "# Your answer BEGINS HERE\n",
        "###\n",
        "\n",
        "# This helper function checks whether the given word \n",
        "# (a) has only one sense (i.e. only one synset), or \n",
        "# (b) where the count (as provided by the WordNet count() method for the lemmas\n",
        "#     associated with a synset) of the most common sense is at least 4 times \n",
        "#     larger than the next most common sense.\n",
        "# (c) has noun as its primary sence\n",
        "def is_single_primarysense(word):\n",
        "\n",
        "    # If the given word has only 1 sense\n",
        "    if len(wordnet.synsets(word)) == 1:\n",
        "        # Check if this sense is a noun, if so consider it valid.\n",
        "        if wordnet.synsets(word)[0].pos() == 'n':\n",
        "            word_primarysense[word] = wordnet.synsets(word)[0]\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    # If the given word has multiple senses    \n",
        "    else:\n",
        "        # Count the number of lemma associated with each sense\n",
        "        counts = []\n",
        "        for synset in wordnet.synsets(word):\n",
        "            count = 0\n",
        "            for lemma in synset.lemmas():\n",
        "                # Consider only the word's own lemma, both the upper and the\n",
        "                # lower cases.\n",
        "                if lemma.name().lower() == word:\n",
        "                    count += lemma.count()\n",
        "            counts.append(count)\n",
        "        primarysense = wordnet.synsets(word)[counts.index(max(counts))] \n",
        "        counts.sort(reverse=True)\n",
        "\n",
        "        # Check if the primary sense has a count at least 4 times greater\n",
        "        # than other senses. \n",
        "        if counts[0] >= (4 * counts[1]) and counts[0] != 0:\n",
        "            # If so, check if this sense is a noun, if so consider it valid.\n",
        "            if primarysense.pos() == 'n':\n",
        "                word_primarysense[word] = primarysense\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "final_dict = {}\n",
        "\n",
        "# Filter out any word (key) pairs where at least one of the two words \n",
        "# does not have single primary sense.\n",
        "for (k, v) in filtered_gold_standard.items():\n",
        "    if is_single_primarysense(k[0]) and is_single_primarysense(k[1]):\n",
        "        final_gold_standard[k] = v\n",
        "\n",
        "###\n",
        "# Your answer ENDS HERE\n",
        "###\n",
        "\n",
        "print(len(final_gold_standard))\n",
        "print(final_gold_standard)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26\n",
            "{('bread', 'butter'): 6.19, ('professor', 'doctor'): 6.62, ('student', 'professor'): 6.81, ('stock', 'egg'): 1.81, ('money', 'cash'): 9.08, ('king', 'queen'): 8.58, ('bishop', 'rabbi'): 6.69, ('football', 'basketball'): 6.81, ('football', 'tennis'): 6.63, ('alcohol', 'chemistry'): 5.54, ('baby', 'mother'): 7.85, ('car', 'automobile'): 8.94, ('journey', 'voyage'): 9.29, ('coast', 'shore'): 9.1, ('brother', 'monk'): 6.27, ('journey', 'car'): 5.85, ('coast', 'hill'): 4.38, ('forest', 'graveyard'): 1.85, ('monk', 'slave'): 0.92, ('coast', 'forest'): 3.15, ('psychology', 'doctor'): 6.42, ('psychology', 'mind'): 7.69, ('psychology', 'health'): 7.23, ('psychology', 'science'): 6.71, ('planet', 'moon'): 8.08, ('planet', 'galaxy'): 8.11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cWgLNH3LcY45"
      },
      "source": [
        "<b>For your testing:</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "th1NNkYZcY45",
        "colab": {}
      },
      "source": [
        "assert(len(final_gold_standard) > 10 and len(final_gold_standard) < 40)\n",
        "assert(final_gold_standard[('professor', 'doctor')] == 6.62)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CnVPXtiLcY5E"
      },
      "source": [
        "## 2. Computing word similiarity with Lin similarity, NPMI and LSA (3 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rSK4x4IBcY5F"
      },
      "source": [
        "### Question 3 (1.0 mark)\n",
        "\n",
        "<b>Instructions</b>: Now you will create several dictionaries with similarity scores for pairs of words in your test set derived using the techniques discussed in class. The first of these is the Lin similarity for your word pairs using the information content of the Brown corpus, which you should calculate using the primary sense for each word derived above. You can use the built-in method included in the NLTK interface, you don't have to implement your own. \n",
        "\n",
        "When you're done, you should store the word pair and similarity mappings in a dictionary called *lin_similarities*.\n",
        "\n",
        "**Task**: Compute word pair similarity using Lin similarity for the test set. Produce *lin_similarities*, a dictionary of word pairs (keys) and computed Lin similarity scores (values).\n",
        "\n",
        "**Check**: Use the assertion statements in *\"For your testing\"* below for the expected *lin_similarities*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "spSqNWwbcY5G",
        "outputId": "a7b8d1e9-244c-48fd-fa3b-e5d7f29b9d39",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from nltk.corpus import wordnet_ic\n",
        "nltk.download('wordnet_ic')\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "\n",
        "# lin_similarities stores the word pair and Lin similarity mappings\n",
        "lin_similarities = {}\n",
        "\n",
        "###\n",
        "# Your answer BEGINS HERE\n",
        "###\n",
        "\n",
        "# Calculate and store the lin similarity for each word pair in final_gold_standard\n",
        "for k in final_gold_standard.keys():\n",
        "    v = word_primarysense[k[0]].lin_similarity(word_primarysense[k[1]], brown_ic)\n",
        "    lin_similarities[k] = v\n",
        "\n",
        "###\n",
        "# Your answer ENDS HERE\n",
        "###\n",
        "\n",
        "print(lin_similarities)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n",
            "{('bread', 'butter'): 0.711420490146294, ('professor', 'doctor'): 0.7036526610448273, ('student', 'professor'): 0.26208607023317687, ('stock', 'egg'): -0.0, ('money', 'cash'): 0.7888839126424345, ('king', 'queen'): 0.25872135992145145, ('bishop', 'rabbi'): 0.6655650900427844, ('football', 'basketball'): 0.7536025025710653, ('football', 'tennis'): 0.7699955045932811, ('alcohol', 'chemistry'): 0.062235427146896456, ('baby', 'mother'): 0.6315913189894092, ('car', 'automobile'): 1.0, ('journey', 'voyage'): 0.6969176573027711, ('coast', 'shore'): 0.9632173804623256, ('brother', 'monk'): 0.24862817480738675, ('journey', 'car'): -0.0, ('coast', 'hill'): 0.5991131628821826, ('forest', 'graveyard'): -0.0, ('monk', 'slave'): 0.2543108201944307, ('coast', 'forest'): -0.0, ('psychology', 'doctor'): -0.0, ('psychology', 'mind'): 0.304017384194818, ('psychology', 'health'): 0.06004979886905243, ('psychology', 'science'): 0.8474590505736942, ('planet', 'moon'): 0.7808882364067532, ('planet', 'galaxy'): -0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GDbnoh-qcY5I"
      },
      "source": [
        "<b>For your testing:</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yH_6wsW6cY5J",
        "colab": {}
      },
      "source": [
        "assert(lin_similarities[('professor', 'doctor')] > 0.5 and lin_similarities[('professor', 'doctor')] < 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m7FXoMudcY5P"
      },
      "source": [
        "### Question 4 (1.0 mark)\n",
        "\n",
        "**Instructions:** Next, you will calculate Normalized PMI (NPMI) for your word pairs using word frequency derived from the Brown.\n",
        "\n",
        "PMI is defined as:\n",
        "\n",
        "\\begin{equation*}\n",
        "PMI = \\log_2\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)\n",
        "\\end{equation*}\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{equation*}\n",
        "p(x,y) = \\frac{\\text{Number of paragraphs with the co-occurrence of x and y}}{\\sum_i \\text{Number of word types in paragraph}_i}\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "p(x) = \\frac{\\text{Number of paragraphs with the occurrence of x}}{\\sum_i \\text{Number of word types in paragraph}_i}\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "p(y) = \\frac{\\text{Number of paragraphs with the occurrence of y}}{\\sum_i \\text{Number of word types in paragraph}_i}\n",
        "\\end{equation*}\n",
        "\n",
        "with the sum over $i$ ranging over all paragraphs. Note that there are other ways PMI could be formulated.\n",
        "\n",
        "NPMI is defined as:\n",
        "\n",
        "\\begin{equation*}\n",
        "NPMI = \\frac{PMI}{-log_2(p(x,y))} = \\frac{log_2(p(x)p(y))}{log_2(p(x,y))} - 1\n",
        "\\end{equation*}\n",
        "\n",
        "Thus, when there is no co-occurrence, NPMI is -1. NPMI is normalized between [-1, +1]. \n",
        "\n",
        "You should use the *brown_corpus* object you've created in question 1 as your corpus here. You need to use the basic method for calculating PMI introduced in class (and also in the reading) which is appropriate for any possible definition of co-occurrence (here, there is co-occurrence when a word pair appears in the same paragraph), but you should only calculate PMI for the words in your test set. You must avoid building the entire co-occurrence matrix, instead you should keeping track of the sums you need for the probabilities as you go along. \n",
        "\n",
        "When you have calculated NPMI for all the pairs, you should store the word pair and NPMI-similarity mappings in a dictionary called *NPMI_similarities*.\n",
        "\n",
        "**Task**: Compute word pair similarity using NPMI similarity for the test set. Produce *NPMI_similarities*, a dictionary of word pairs (keys) and computed NPMI similarity scores (values).\n",
        "\n",
        "**Check**: Use the assertion statements in *\"For your testing\"* below for the expected *NPMI_similarities*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1yoPO9BecY5Q",
        "outputId": "61fc8314-757f-45c9-8936-41b981e3d22d",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# NPMI_similarities stores the word pair and NPMI similarity mappings\n",
        "NPMI_similarities = {}\n",
        "\n",
        "###\n",
        "# Your answer BEGINS HERE\n",
        "###\n",
        "import math\n",
        "\n",
        "# This helper function calculates the NPMI similarity between a word pair based\n",
        "# on the brown_corpus.\n",
        "def calculate_NPMI(x, y):\n",
        "    x_count = 0\n",
        "    y_count = 0\n",
        "    joint_count = 0\n",
        "    total_count = 0\n",
        "    for paragraph in brown_corpus:\n",
        "        # Calculate the number of word types in the current paragraph\n",
        "        total_count += len(paragraph)\n",
        "\n",
        "        # Check the existence of word x in the current paragraph\n",
        "        if x in paragraph:\n",
        "            x_count += 1\n",
        "\n",
        "        # Check the existence of word y in the current paragraph                   \n",
        "        if y in paragraph:\n",
        "            y_count += 1\n",
        "\n",
        "        # Calculate the co-occurence of word x and y in the current paragraph\n",
        "        if x in paragraph and y in paragraph:\n",
        "            joint_count += 1\n",
        "    \n",
        "    if joint_count == 0:\n",
        "        return -1\n",
        "    return (math.log2((x_count/total_count) * (y_count/total_count)) / math.log2(joint_count/total_count)) - 1\n",
        "\n",
        "# Calculate and store the NPMI similarity for each word pair in final_gold_standard\n",
        "for k in final_gold_standard.keys():\n",
        "    v = calculate_NPMI(k[0], k[1])\n",
        "    NPMI_similarities[k] = v\n",
        "\n",
        "###\n",
        "# Your answer ENDS HERE\n",
        "###\n",
        "\n",
        "print(NPMI_similarities)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('bread', 'butter'): 0.6531272737497535, ('professor', 'doctor'): -1, ('student', 'professor'): 0.535495995400926, ('stock', 'egg'): 0.3685128922213745, ('money', 'cash'): 0.44493834723517245, ('king', 'queen'): 0.4181407297139583, ('bishop', 'rabbi'): -1, ('football', 'basketball'): 0.7161994042283006, ('football', 'tennis'): -1, ('alcohol', 'chemistry'): 0.6246376972254175, ('baby', 'mother'): 0.5149353890388502, ('car', 'automobile'): 0.5430334549802616, ('journey', 'voyage'): -1, ('coast', 'shore'): 0.5861510629843374, ('brother', 'monk'): 0.42993185256931743, ('journey', 'car'): -1, ('coast', 'hill'): 0.33931028834846244, ('forest', 'graveyard'): -1, ('monk', 'slave'): -1, ('coast', 'forest'): 0.45787396913892886, ('psychology', 'doctor'): 0.4613281583726412, ('psychology', 'mind'): 0.4461666043435182, ('psychology', 'health'): -1, ('psychology', 'science'): 0.5908740096534864, ('planet', 'moon'): 0.658000616232332, ('planet', 'galaxy'): -1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1uJbBXzfcY5S"
      },
      "source": [
        "<b>For your testing:</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-nhg3YOBcY5S",
        "colab": {}
      },
      "source": [
        "assert(NPMI_similarities[('professor', 'doctor')] == -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hfkyiBkcY5a"
      },
      "source": [
        "### Question 5 (1.0 mark)\n",
        "\n",
        "**Instructions:** Here we'll use singular value decomposition (SVD) to derive similarity scores using the Latent Semantic Analysis (LSA) method. Recall that LSA applies SVD and truncation to get a dense vector representation of a word type. To measure similarity between two words we calculate cosine similarity between the LSA vectors of the words.\n",
        "\n",
        "We'll first build a term-document frequency matrix (as before, a document is a paragraph in Brown corpus). That is, the rows corresponds to words in the vocabulary, and the columns represent the paragraphs/documents in Brown corpus. Each cell records the document frequency of a word (1 if the word appears in the document, 0 otherwise). You should use the *brown_corpus* object created in question 1 as your corpus to build the term-document frequency matrix.\n",
        "\n",
        "Given the term-document frequency matrix, we'll use `truncatedSVD` in `sklearn` to produce dense vectors of length k = 500, and then use cosine similarity to produce similarities for the word pairs in the test set.\n",
        "\n",
        "You can use the workshop's notebook on distributional similarity (``09-distributional-similarity.ipynb``) as a starting point, but note that we use term-document frequency matrix (as opposed to the tf-idf matrix in the workshop).\n",
        "\n",
        "When you are done, you should store the word pair and LSA-similarity mappings in a dictionary called *LSA_similarities*.\n",
        "\n",
        "**Task**: Compute word pair similarity using LSA. Produce *LSA_similarities*, a dictionary of word pairs (keys) and computed LSA similarity scores (values).\n",
        "\n",
        "**Check**: Use the assertion statements in *\"For your testing\"* below for the expected *LSA_similarities*. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5qg8WfjVcY5b",
        "outputId": "796448e4-959d-4f85-e20a-a32fe488317c",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# LSA_similarities stores the word pair and LSA similarity mappings\n",
        "LSA_similarities = {}\n",
        "\n",
        "###\n",
        "# Your answer BEGINS HERE\n",
        "###\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.spatial.distance import cosine as cos_distance\n",
        "\n",
        "vectorizer = DictVectorizer()\n",
        "svd = TruncatedSVD(n_components=500, random_state=1)\n",
        "\n",
        "# Build a document-term frequency matrix based on the brown_corpus\n",
        "brown_matrix = vectorizer.fit_transform([dict.fromkeys(paragraph, 1) for paragraph in brown_corpus])\n",
        "# Transpose the document-term frequency matrix to a term-document frequency \n",
        "# matrix and truncate each row of the document-term frequency matrix to dense \n",
        "# vectors of length k = 500.\n",
        "brown_matrix = svd.fit_transform(brown_matrix.transpose())\n",
        "\n",
        "# Calculate and store the LSA similarity for each word pair in final_gold_standard\n",
        "for k in final_gold_standard.keys():\n",
        "    vector_1, vector_2 = vectorizer.transform({k[0]:1}), vectorizer.transform({k[1]:1})\n",
        "    LSA_similarities[k] = 1 - cos_distance(vector_1 * brown_matrix, vector_2 * brown_matrix)\n",
        "\n",
        "###\n",
        "# Your answer ENDS HERE\n",
        "###\n",
        "\n",
        "print(LSA_similarities)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('bread', 'butter'): 0.3152754702572451, ('professor', 'doctor'): 0.05844298794179126, ('student', 'professor'): 0.2767337329493138, ('stock', 'egg'): 0.14436308551189037, ('money', 'cash'): 0.15554334593699293, ('king', 'queen'): 0.1302217523561776, ('bishop', 'rabbi'): 0.031084205148395827, ('football', 'basketball'): 0.24386084983005551, ('football', 'tennis'): 0.12632217011903002, ('alcohol', 'chemistry'): 0.07876794799305287, ('baby', 'mother'): 0.35791140240818753, ('car', 'automobile'): 0.3600832277825228, ('journey', 'voyage'): 0.10533899235161615, ('coast', 'shore'): 0.40214077624750144, ('brother', 'monk'): 0.11876162813444846, ('journey', 'car'): 0.001960165045462592, ('coast', 'hill'): 0.22113053499136226, ('forest', 'graveyard'): 0.08874563656026124, ('monk', 'slave'): -0.030531133573225677, ('coast', 'forest'): 0.13886784050556522, ('psychology', 'doctor'): 0.14332368812525242, ('psychology', 'mind'): 0.13327874870626477, ('psychology', 'health'): 0.027449699898818847, ('psychology', 'science'): 0.24664309131634288, ('planet', 'moon'): 0.42874501414192934, ('planet', 'galaxy'): 0.031230836291777697}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dLtjcNgIcY5d"
      },
      "source": [
        "<b>For your testing:</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZ0sUZbNcY5e",
        "colab": {}
      },
      "source": [
        "assert(LSA_similarities[('professor', 'doctor')] > 0 and LSA_similarities[('professor', 'doctor')] < 0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDyzdGHjQluB",
        "colab_type": "text"
      },
      "source": [
        "## Computing word similarity with feedforward language model (3 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVgRBEtDQluC",
        "colab_type": "text"
      },
      "source": [
        "### Question 6 (1.0 mark)\n",
        "\n",
        "**Instructions**: Here we'll build a n-gram neural language model to learn word embeddings, and compute word similarity based on the word embeddings. As before we will use the Brown corpus as training data for our language model, and the first step is to collect the vocabulary.\n",
        "\n",
        "As before, we'll treat paragraphs in the Brown corpus as our documents. The first 12K paragraphs/documents will serve as our training data, and the rest (3K+ documents) as development data. The first step towards building a language model is to collect the vocabulary, i.e. the set of unique word types in our training data. When collecting the word types, you should lowercase all words, and only keep word types that have a frequency $>= 5$. Store your vocabulary in the _vocab_ object.\n",
        "\n",
        "Note: \n",
        "  - we'll be using _$<$UNK$>$_ to represent unseen words, and so _vocab_ is initialised with the special _$<$UNK$>$_.\n",
        "  - you do not need to do any other additional preprocessing aside from the aforementioned steps (e.g. no need to remove symbols, etc)\n",
        "  - you should not use *brown_corpus* here, as you need the words in their original order (*brown_corpus* stores a set of words for each paragraph, and so do not contain word order information)\n",
        "\n",
        "**Task**: Collect a set of unique word types in the training portion (first 12K paragraphs) of the Brown corpus. Produce _vocab_, which is a set that contains all the word types.\n",
        "\n",
        "**Check**: Use the assertion statements in *\"For your testing\"* below to check the vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u2HdOXvYcY5k",
        "outputId": "f6e71aef-0249-4c73-c9f7-f312fdd3e46c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_train = 12000\n",
        "UNK_symbol = \"<UNK>\"\n",
        "vocab = set([UNK_symbol])\n",
        "word_dict = {}\n",
        "###\n",
        "# Your answer BEGINS HERE\n",
        "###\n",
        "\n",
        "# Collect a dictionary of lowercased words and their number of occurrences in\n",
        "# the original Brown corpus.\n",
        "for paragraph in brown.paras()[0:12000]:\n",
        "    for sentence in paragraph:\n",
        "        for word in sentence:\n",
        "            if word.lower() in word_dict.keys():\n",
        "                word_dict[word.lower()] += 1\n",
        "            else:\n",
        "                word_dict[word.lower()] = 1        \n",
        "\n",
        "# Filter out words which appears less than 4 times and store the rest to the\n",
        "# vocabulary set.\n",
        "for k, v in word_dict.items():\n",
        "    if v >= 5:\n",
        "        vocab.add(k)\n",
        "                \n",
        "###\n",
        "# Your answer ENDS HERE\n",
        "###\n",
        "\n",
        "print(len(vocab))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8syrr-aeQluG",
        "colab_type": "text"
      },
      "source": [
        "**For your testing:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE_F9-AuQluH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert(len(vocab) > 8000 and len(vocab) < 20000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46wZfIGTQluJ",
        "colab_type": "text"
      },
      "source": [
        "### Question 7 (1.0 mark)\n",
        "\n",
        "**Instructions**: As we'll be building a trigram neural language model (based on lecture 7, page 20), the next step is to collect trigrams to construct our training data. In a trigram neural language model, for example if we have the trigram _cow eats grass_, the input to the model is the first two terms of a trigram (_cow_ and _eats_), and the language model's aim is to predict the last term of the trigram (_grass_). Your task here is to construct the training and development data for the language model. Just like the previous step, the first 12K paragraphs will serve as our training data, and the remaining 3K+ will be for development. You'll need to map words into IDs when constructing the training and development data. Any words that are not in _vocab_ should be mapped to the special _$<$UNK$>$_ symbol.\n",
        "\n",
        "As an example, given the sentence \"_a big fat hungry cow ._\", you should create the following training examples:\n",
        "\n",
        "|input|target|\n",
        "|:---:|:----:|\n",
        "|<i>a</i>, _big_|_fat_|\n",
        "|_big_, _fat_|_hungry_|\n",
        "|_fat_, _hungry_|_cow_|\n",
        "|_hungry_, _cow_|_._|\n",
        "\n",
        "\n",
        "Note:\n",
        "   - _vocab_ is a set and so does not map words into IDs. You'll need to create a word to ID mapping first based on _vocab_.\n",
        "   - A trigram should not cross sentence boundary.\n",
        "   - You should ignore sentences that have less than 3 words (as they are too short to form trigrams).\n",
        "   - We won't need special starting symbol when collecting the trigrams, as we are only interested in learning word embeddings here (and not computing probabilities of a sentence).\n",
        "\n",
        "\n",
        "**Task**: Create training and development data. The training input and target data should be stored in the <i>x_train</i> and <i>y_train</i> respectively, and development input and target data in <i>x_dev</i> and <i>y_dev</i> respectively.\n",
        "\n",
        "**Check**: Use the assertion statements in *\"For your testing\"* below for the expected shape of the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZeDsiZ6QluJ",
        "colab_type": "code",
        "outputId": "2489338d-5ae1-4cd9-e9e8-475a729be94c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "###\n",
        "# Your answer BEGINS HERE\n",
        "###\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from itertools import tee\n",
        "\n",
        "# Build an index table for each word in vocab\n",
        "word2idx = {}\n",
        "i = 0\n",
        "for word in vocab:\n",
        "    word2idx[word] = i\n",
        "    i += 1\n",
        "\n",
        "# This helper function iterates over each sentence in a \n",
        "# (w0, w1, w2)->(w1, w2, w3)->(w2, w3, w4)... manner.\n",
        "def triwise(iterable):\n",
        "    a, b = tee(iterable)\n",
        "    a, c = tee(iterable)\n",
        "    next(b, None)\n",
        "    next(c, None)\n",
        "    next(c, None)\n",
        "    return zip(a, b, c)\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_dev = []\n",
        "y_dev = []\n",
        "\n",
        "# Construct the training set\n",
        "for paragraph in brown.paras()[:12000]:\n",
        "    for sentence in paragraph:\n",
        "        # Ignores sentences with length < 3.\n",
        "        if len(sentence) < 3:\n",
        "            continue\n",
        "        else:\n",
        "            for a, b, c in triwise(sentence):\n",
        "                a, b, c = a.lower(), b.lower(), c.lower()\n",
        "                # If the word is not in vocab, converts it to <UNK> instead.\n",
        "                train_1 = word2idx[a] if a in vocab else word2idx[\"<UNK>\"]\n",
        "                train_2 = word2idx[b] if b in vocab else word2idx[\"<UNK>\"]\n",
        "                target = word2idx[c] if c in vocab else word2idx[\"<UNK>\"]\n",
        "                x_train.append((train_1, train_2))\n",
        "                y_train.append(target)\n",
        "\n",
        "# Construct the development set\n",
        "for paragraph in brown.paras()[12000:]:\n",
        "    for sentence in paragraph:\n",
        "        if len(sentence) < 3:\n",
        "            continue      \n",
        "        else:\n",
        "            for a, b, c in triwise(sentence):\n",
        "                a, b, c = a.lower(), b.lower(), c.lower()\n",
        "                # If the word is not in vocab, converts it to <UNK> instead.                \n",
        "                dev_1 = word2idx[a] if a in vocab else word2idx[\"<UNK>\"]\n",
        "                dev_2 = word2idx[b] if b in vocab else word2idx[\"<UNK>\"]\n",
        "                target = word2idx[c] if c in vocab else word2idx[\"<UNK>\"]\n",
        "                x_dev.append((dev_1, dev_2))\n",
        "                y_dev.append(target)\n",
        "    \n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_dev = np.array(x_dev)\n",
        "y_dev = np.array(y_dev)\n",
        "\n",
        "###\n",
        "# Your answer ENDS HERE\n",
        "###\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_dev.shape)\n",
        "print(y_dev.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(872823, 2)\n",
            "(872823,)\n",
            "(174016, 2)\n",
            "(174016,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4n4tZeAQluR",
        "colab_type": "text"
      },
      "source": [
        "**For your testing:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYuYdbMCQluR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert(x_train.shape[0] == y_train.shape[0])\n",
        "assert(x_dev.shape[0] == y_dev.shape[0])\n",
        "assert(x_train.shape[0] > 500000)\n",
        "assert(x_dev.shape[0] > 50000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtSVyQO6QluT",
        "colab_type": "text"
      },
      "source": [
        "### Question 8 (1.0 mark)\n",
        "\n",
        "**Instructions**: Now let's build the trigram neural language model. We'll use the language model described in lecture 7 (page 20):\n",
        "\n",
        "$x' = e(x_1) \\oplus e(x_2)$\n",
        "\n",
        "$h = \\tanh(W_1 x' + b)$\n",
        "\n",
        "$y = $ softmax$(W_2 h)$\n",
        "\n",
        "where $\\oplus$ is the concatenation operation, $x_1$ and $x_2$ are the input words, $e$ is an embedding function, and $y$ is the target word. You can use either `keras` or `pytorch` for building your model.\n",
        "\n",
        "Set the dimension of the word embeddings and $h$ to 100, and train your model with 3 epochs with a batch size of 256. You will not need to tune hyper-parameters for this task.\n",
        "\n",
        "After the model is trained, use the word embeddings to compute word similarity in the test set. Store the similarity scores in a dictionary called *lm_similarities*.\n",
        "\n",
        "Note:\n",
        "  - For words in the test set that are not in your vocabulary, you should treat them as unknown words. In other words, you should use $<$UNK$>$'s embedding to represent these words.\n",
        "  - The training may take some time on CPU. You can run your notebook on colab with a GPU if you want faster training (see the programming exercise in workshop-07 if you're not familiar with colab)\n",
        "\n",
        "**Task**: Train a trigram neural language model, and use the learned word embeddings to compute word similarity. Produce *lm_similarities*, a dictionary that contains word pairs as keys and similarity scores as values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlVRe-opcY5n",
        "outputId": "287ab838-4c3d-467f-f5d6-2e0d3630849f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "lm_similarities = {}\n",
        "from keras.layers import Dense, Input, Embedding, Flatten\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "###\n",
        "# Your answer BEGINS HERE\n",
        "###\n",
        "\n",
        "# Declare a embedding layer with the input dimension being the number of\n",
        "# words in vocab and the output dimension being 100. The trainable option\n",
        "# is set to be on.\n",
        "embedding_layer = Embedding(\n",
        "    len(vocab),\n",
        "    100,\n",
        "    trainable=True\n",
        ")\n",
        "\n",
        "# Set the input shape to be 2 to take in the bi-gram inputs.\n",
        "input_ = Input(shape=(2,))\n",
        "x = embedding_layer(input_)\n",
        "\n",
        "# Concatenate the word embedding for each word in the bi-gram pair.\n",
        "x = Flatten()(x)\n",
        "\n",
        "# The h=tanh(W1xâ€²+b) layer with 100 units\n",
        "x = Dense(100, activation='tanh')(x)\n",
        "\n",
        "# Output layer\n",
        "x = Dense(len(list(vocab)), activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_, output=x)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=Adam(),\n",
        "    metrics=['accuracy', 'cosine_similarity']\n",
        ")\n",
        "\n",
        "# Train the model for 3 epochs with a batch size of 256.\n",
        "r = model.fit(\n",
        "  x_train,\n",
        "  y_train,\n",
        "  batch_size=256,\n",
        "  epochs=3,\n",
        "  validation_data = (x_dev, y_dev)\n",
        ")\n",
        "\n",
        "# Extract the word embedding weight\n",
        "embeddings = embedding_layer.get_weights()[0]\n",
        "words_embeddings = {w: embeddings[idx] for w, idx in word2idx.items()}\n",
        "\n",
        "# Calculate the cosine similarity for each pair of words in the final_gold_standard\n",
        "# based on the trained word embedding vectors\n",
        "for k in final_gold_standard.keys():\n",
        "    word1 = k[0]\n",
        "    word2 = k[1]\n",
        "    if k[0] not in vocab:\n",
        "        word1 = \"<UNK>\"\n",
        "    if k[1] not in vocab:\n",
        "        word2 = \"<UNK>\"\n",
        "    lm_similarities[k] = 1 - cos_distance(words_embeddings[word1], words_embeddings[word2])\n",
        "\n",
        "###\n",
        "# Your answer ENDS HERE\n",
        "###\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 872823 samples, validate on 174016 samples\n",
            "Epoch 1/3\n",
            "872823/872823 [==============================] - 30s 34us/step - loss: 5.9388 - accuracy: 0.1496 - cosine_similarity: 9.0919 - val_loss: 5.2474 - val_accuracy: 0.1667 - val_cosine_similarity: 5.9600\n",
            "Epoch 2/3\n",
            "872823/872823 [==============================] - 29s 33us/step - loss: 5.4313 - accuracy: 0.1839 - cosine_similarity: 5.8725 - val_loss: 5.1332 - val_accuracy: 0.1727 - val_cosine_similarity: 5.3007\n",
            "Epoch 3/3\n",
            "872823/872823 [==============================] - 29s 33us/step - loss: 5.2302 - accuracy: 0.1955 - cosine_similarity: 5.6881 - val_loss: 5.0848 - val_accuracy: 0.1770 - val_cosine_similarity: 5.1381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AejjfsPBkqg",
        "colab_type": "code",
        "outputId": "a8eafcd0-581e-4bec-940b-3b9f3bbbfe6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(lm_similarities)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('bread', 'butter'): 0.23464332520961761, ('professor', 'doctor'): 0.42521747946739197, ('student', 'professor'): 0.5182636976242065, ('stock', 'egg'): 0.2642689645290375, ('money', 'cash'): 0.24615910649299622, ('king', 'queen'): 0.5198251008987427, ('bishop', 'rabbi'): 0.30148082971572876, ('football', 'basketball'): 0.4315166771411896, ('football', 'tennis'): 0.38201990723609924, ('alcohol', 'chemistry'): 0.06805875152349472, ('baby', 'mother'): 0.3200019896030426, ('car', 'automobile'): 0.011390027590095997, ('journey', 'voyage'): 0.3481979966163635, ('coast', 'shore'): 0.055063918232917786, ('brother', 'monk'): 0.3916759192943573, ('journey', 'car'): 0.3893755078315735, ('coast', 'hill'): 0.2864517867565155, ('forest', 'graveyard'): -0.12595079839229584, ('monk', 'slave'): -0.057303737848997116, ('coast', 'forest'): 0.5638205409049988, ('psychology', 'doctor'): 0.35006019473075867, ('psychology', 'mind'): 0.07453189045190811, ('psychology', 'health'): 0.3338579833507538, ('psychology', 'science'): 0.2703481614589691, ('planet', 'moon'): 0.39964836835861206, ('planet', 'galaxy'): -0.0018128649098798633}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qTgtO9-pcY5q"
      },
      "source": [
        "## 3. Comparison with the Gold Standard (1 mark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5UpgjOKmcY5r"
      },
      "source": [
        "### Question 9 (1.0 mark)\n",
        "\n",
        "**Instructions:** Finally, you should compare all the similarities you've created to the gold standard you loaded and filtered in the first step. For this, you can use the Pearson correlation co-efficient (`pearsonr`), which is included in scipy (`scipy.stats`). Be careful converting your dictionaries to lists for this purpose, the data for the two datasets needs to be in the same order for correct comparison using correlation. Write a general function, then apply it to each of the similarity score dictionaries.\n",
        "\n",
        "When you are done, you should put the result in a dictionary called *pearson_correlations* (use the keys: 'lin', 'NPMI', 'LSA', 'lm').\n",
        "\n",
        "**Task**: Compute the Pearson correlation coefficient between the estimated similarity scores (Lin, NPMI and LSA similarities) and the gold standard similarity ratings. Produce *pearson_correlations*, a dictionary containing the methods as keys and correlations as values.\n",
        "\n",
        "**Check**: Use the assertion statements in *\"For your testing\"* below for the expected *pearson_correlations*. \n",
        "\n",
        "<b>Hint:</b> All of the methods used here should be markedly above 0, but also far from 1 (perfect correlation); if you're not getting reasonable results, go back and check your code for bugs! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oAgJwXhxcY5r",
        "outputId": "579c643f-4fd1-4ec0-8744-12504e4d5acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# pearson_correlations stores the pearson correlations with the gold standard of 'lin', 'NPMI', 'LSA', 'lm'\n",
        "pearson_correlations = {}\n",
        "\n",
        "###\n",
        "# Your answer BEGINS HERE\n",
        "###\n",
        "lin_list = []\n",
        "NPMI_list = []\n",
        "LSA_list = []\n",
        "lm_list = []\n",
        "final_gold_standard_list = []\n",
        "\n",
        "# Sort the vector for each similarity dictionaries so that they are in the\n",
        "# same order.\n",
        "for k in final_gold_standard:\n",
        "    lin_list.append(lin_similarities[k])\n",
        "    NPMI_list.append(NPMI_similarities[k])\n",
        "    LSA_list.append(LSA_similarities[k])\n",
        "    lm_list.append(lm_similarities[k])\n",
        "    final_gold_standard_list.append(final_gold_standard[k])\n",
        "\n",
        "# Calculate the pearson_correlations between each similarity vector and the\n",
        "# final_gold_standard_list.\n",
        "pearson_correlations['lin'] = pearsonr(lin_list, final_gold_standard_list)[0]\n",
        "pearson_correlations['NPMI'] = pearsonr(NPMI_list, final_gold_standard_list)[0]\n",
        "pearson_correlations['LSA'] = pearsonr(LSA_list, final_gold_standard_list)[0]\n",
        "pearson_correlations['lm'] = pearsonr(lm_list, final_gold_standard_list)[0]\n",
        "    \n",
        "###\n",
        "# Your answer ENDS HERE\n",
        "###\n",
        "\n",
        "print(pearson_correlations)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lin': 0.5301489978447533, 'NPMI': 0.18799888869410047, 'LSA': 0.3888687679346686, 'lm': 0.19255380954350473}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aV6PxadqcY5v"
      },
      "source": [
        "<b>For your testing:</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q2fNHrT0cY5v",
        "colab": {}
      },
      "source": [
        "assert(pearson_correlations['lin'] > 0.4 and pearson_correlations['lin'] < 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oIoExaG0cY51"
      },
      "source": [
        "## A final word\n",
        "\n",
        "Normally, we would not use a corpus as small as the Brown for the purposes of building word vectors. Also, note that filtering our test set to just words we are likely to do well on would typically be considered cheating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aCwCHklbcY52",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}